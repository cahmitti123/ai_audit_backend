services:
  redis:
    image: redis:7-alpine
    restart: always
    # No host port published by default (internal-only). If you need local access,
    # temporarily add: ports: ["127.0.0.1:${REDIS_PORT:-6379}:6379"]
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis-data:/data
    networks:
      - ai-audit-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Internal load balancer for the `server` replicas.
  # Publish this port and point your host nginx / domain at it.
  lb:
    image: nginx:1.27-alpine
    restart: always
    ports:
      # Bind to localhost; put your public Nginx in front for :80/:443.
      - "127.0.0.1:${SERVER_PORT:-3002}:80"
    volumes:
      - ./deploy/nginx-lb.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - ai-audit-network
    depends_on:
      - server
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Main API Server (scale this to 3 replicas)
  # Usage: `docker compose -f docker-compose.prod.scale.yml up -d --build --scale server=3`
  server:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    restart: always
    environment:
      - NODE_ENV=production
      - PORT=3002
      - DATABASE_URL=${DATABASE_URL}
      - DIRECT_URL=${DIRECT_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
      - FICHE_API_BASE_URL=${FICHE_API_BASE_URL}
      - FICHE_API_AUTH_TOKEN=${FICHE_API_AUTH_TOKEN}
      - INNGEST_EVENT_KEY=${INNGEST_EVENT_KEY}
      - INNGEST_SIGNING_KEY=${INNGEST_SIGNING_KEY}
      - INNGEST_BASE_URL=http://inngest:8288
      - INNGEST_DEV=0
      - FRONTEND_WEBHOOK_URL=${FRONTEND_WEBHOOK_URL}
      - WEBHOOK_SECRET=${WEBHOOK_SECRET}
      - WEBHOOK_TIMEOUT=${WEBHOOK_TIMEOUT:-10000}
      - WEBHOOK_MAX_ATTEMPTS=${WEBHOOK_MAX_ATTEMPTS:-3}
      - WEBHOOK_ALLOWED_ORIGINS=${WEBHOOK_ALLOWED_ORIGINS}
      - VECTOR_STORE_ID=${VECTOR_STORE_ID}
      - VECTOR_STORE_MAX_RESULTS=${VECTOR_STORE_MAX_RESULTS:-5}
      - REDIS_URL=${REDIS_URL:-redis://redis:6379}
      - OPENAI_MODEL_AUDIT=${OPENAI_MODEL_AUDIT:-gpt-5.2}
      - OPENAI_MODEL_CHAT=${OPENAI_MODEL_CHAT:-gpt-5.2}
      - OPENAI_TEMPERATURE_CHAT=${OPENAI_TEMPERATURE_CHAT:-0}
      - AUDIT_EVIDENCE_GATING=${AUDIT_EVIDENCE_GATING:-1}
      - PRODUCT_VECTORSTORE_FALLBACK=${PRODUCT_VECTORSTORE_FALLBACK:-0}
      - AUDIT_STEP_TIMELINE_EXCERPT=${AUDIT_STEP_TIMELINE_EXCERPT:-1}
      - AUDIT_STEP_TIMELINE_MAX_CHUNKS=${AUDIT_STEP_TIMELINE_MAX_CHUNKS:-40}
      - AUTOMATION_SCHEDULER_CRON=${AUTOMATION_SCHEDULER_CRON:-*/1 * * * *}
      - AUTOMATION_SCHEDULER_WINDOW_MINUTES=${AUTOMATION_SCHEDULER_WINDOW_MINUTES:-20}
      # Inngest scaling:
      # - INNGEST_PARALLELISM_PER_SERVER: how many tasks each server replica can run in parallel (default 10)
      # - INNGEST_SERVER_REPLICAS: how many `server` containers you scaled to (ex: --scale server=6)
      - INNGEST_PARALLELISM_PER_SERVER=${INNGEST_PARALLELISM_PER_SERVER:-10}
      - INNGEST_SERVER_REPLICAS=${SERVER_REPLICAS:-6}
    volumes:
      - audit-data:/app/data
    networks:
      - ai-audit-network
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "require('http').get('http://localhost:3002/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Inngest Self-Hosted Server (single instance)
  # IMPORTANT: point sdk-url at the load balancer so invocations distribute across replicas.
  inngest:
    image: inngest/inngest:latest
    restart: always
    ports:
      # Bind to localhost (access UI via SSH tunnel in prod).
      - "127.0.0.1:${INNGEST_PORT:-8288}:8288"
      - "127.0.0.1:${INNGEST_CONNECT_PORT:-8289}:8289"
    environment:
      - INNGEST_EVENT_KEY=${INNGEST_EVENT_KEY}
      - INNGEST_SIGNING_KEY=${INNGEST_SIGNING_KEY}
      - INNGEST_LOG_LEVEL=info
      - INNGEST_POLL_INTERVAL=60
    command: inngest start --sdk-url http://lb/api/inngest
    networks:
      - ai-audit-network
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8288/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      lb:
        condition: service_started
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ai-audit-network:
    driver: bridge

volumes:
  audit-data:
    driver: local
  redis-data:
    driver: local


